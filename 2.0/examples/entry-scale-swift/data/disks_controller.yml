#
# (c) Copyright 2015 Hewlett Packard Enterprise Development Company LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2

  disk-models:
  - name: CONTROLLER-DISKS

    # This example is based on using a single 500GB disk for a volume
    # group that contains all file systems on a controller with 64GB
    # of memory. If the disk is less than 500GB then you may need to
    # increase the percentage of the volume group allocated to the root
    # logical volume.
    #
    # Additional disks can be added to the 'physical-volumes' section.
    # 
    # If the available capacity of your servers is more that 500GB you
    # should consider using the "CONTROLLER_1TB-DISKS" disk-model
    # in disks_controller_1TB.yml instead.  To use this alternative model
    # you need to edit the CONTROLLER-ROLE sections of server_roles.yml
    #

    volume-groups:
      - name: hlm-vg
        physical-volumes:

          # NOTE: 'sda_root' is a templated value. This value is checked in
          # os-config and replaced by the partition actually used on sda
          #e.g. sda1 or sda5
          - /dev/sda_root

          # Add any additional disks for the volume group here
          # -/dev/sdx
          # -/dev/sdy

        logical-volumes:
          # The policy is not to consume 100% of the space of each volume group.
          # 5% should be left free for snapshots and to allow for some flexibility.

          - name: root
            size: 10%
            fstype: ext4
            mount: /

          # Reserved space for kernel crash dumps
          # Should evaluate to a value that is slightly larger that
          # the memory size of your server
          - name: crash
            size: 13%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file

          # Local Log files.
          - name: log
            size: 13%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file

          # Mysql Database.  All persistent state from OpenStack services
          # is saved here.  Although the individual objects are small the
          # accumulated data can grow over time
          - name: mysql
            size: 10%
            mount: /var/lib/mysql
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: mysql

          # Rabbitmq works mostly in memory, but needs to be able to persist
          # messages to disc under high load. This area should evaluate to a value
          # that is slightly larger than the memory size of your server
          - name: rabbitmq
            size: 13%
            mount: /var/lib/rabbitmq
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: rabbitmq
              rabbitmq_env: home

          # Database storage for event monitoring (Monasca).  Events are generally
          # small data objects.
          - name: vertica
            size: 3%
            mount: /var/vertica
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: vertica

          # Messaging system for monitoring.
          - name: kafka
            size: 2%
            mount: /var/kafka
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: kafka

          # Data storage for centralized logging. This holds log entries from all
          # servers in the cloud and hence can require a lot of disk space.
          - name: elasticsearch
            size: 30%
            mount: /var/lib/elasticsearch
            fstype: ext4

          # Zookeeper is used to provide cluster co-ordination in the monitoring
          # system.  Although not a high user of disc space we have seen issues 
          # with zookeeper snapshots filling up filesystems so we keep it in its
          # own space for stability. 
          - name: zookeeper
            size: 1%
            mount: /var/lib/zookeeper
            fstype: ext4

        consumer:
           name: os

    # Additional disk group defined for Swift
    # If available, you can add additional disks to the "devices" list.
    # Only list disks that are present at deployment time.
    device-groups:
      - name: swiftpac
        devices:
          - name: /dev/sdb
          - name: /dev/sdc
          # Add any additional disks for swift here
          #- name: /dev/sdd
          #- name: /dev/sde
        consumer:
          name: swift
          attrs:
            rings:
              - account
              - container
